{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data for Data Science 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "Import a set of packages that we will use in order to perform text analysis. These are very commonly used Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # The best known Python natural language processing toolkit\n",
    "import matplotlib # Python plotting library\n",
    "import matplotlib.pyplot as plt # Easy syntax access to pyplot\n",
    "%matplotlib inline \n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "import requests # for URL requests\n",
    "from bs4 import BeautifulSoup # Import the BeautifulSoup package for web scraping\n",
    "import feedparser  # For reading RSS feeds - imstnall using ./conda install feedparser\n",
    "import tweepy # for access to the twitter apis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Accessing Data From Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing data from a text file is straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple text file into a string variable\n",
    "f = open(\"data/test_text.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Iterate through each line in the file and print it out\n",
    "for line in nltk.sent_tokenize(lines[0]):\n",
    "    print(\"****\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for line in nltk.sent_tokenize(lines[0]):\n",
    "    print(\"****\")\n",
    "    for word in nltk.word_tokenize(line):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even load a text file across the Internet by using **requests.get** from the **requests** package instead of simply **open**. We use [the Guttenberg Press](http://www.gutenberg.org) in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a URL to Alice in Wonderland on the Guttenberg Press (www.gutenberg.org)\n",
    "url='http://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "\n",
    "# Read the text from the URL\n",
    "text_page = requests.get(url).text\n",
    "\n",
    "# Print the first 500 characters of the book\n",
    "print(text_page[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even connnect to a HTML file, but this starts to get messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to a URL and extract the HTML text\n",
    "url = \"http://www.independent.ie/sport/soccer/international-soccer/neil-taylor-facing-longer-ban-for-seamus-coleman-horror-tackle-as-fifa-step-in-35578919.html\"\n",
    "text = requests.get(url).text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing data from web pages is straightforward. The tricky bit is extracting the useful information from the webpage. We can use the **BeautifulSoup4** (http://www.crummy.com/software/BeautifulSoup) packages to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the HTML file\n",
    "url = \"http://www.independent.ie/sport/soccer/international-soccer/neil-taylor-facing-longer-ban-for-seamus-coleman-horror-tackle-as-fifa-step-in-35578919.html\"\n",
    "html = requests.get(url).text\n",
    "\n",
    "# Create a beautiful soup object from the text file so that we get at the article text\n",
    "article_soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract the actual article text  - this relies on the fact that I know what the HTML looks like, not completely robust!\n",
    "article = article_soup.find('article')\n",
    "headline = article.find('h1')\n",
    "article_content = article.find_all('p')\n",
    "\n",
    "# Start the article text by adding the headline\n",
    "article_text = headline.get_text()\n",
    "\n",
    "# Construct the article by adding togehter the paragraph pieces\n",
    "for tag in article_content:\n",
    "    article_text += tag.get_text()\n",
    "    \n",
    "# Print the article content\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing RSS Feeds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to access lots of news articles is to use an RSS feed. We can access RSS feeds easily in Python using the **feedparser** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read from the Irish Times RSS feed\n",
    "RSS_url = \"https://www.irishtimes.com/cmlink/news-1.1319192\"\n",
    "it_feed = feedparser.parse(RSS_url)\n",
    "len(it_feed.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the entries from the feed and print the title of each article and the URL for the articl\n",
    "for article_entry in it_feed.entries:\n",
    "    article_title = article_entry['title']\n",
    "    article_url = article_entry['links'][0]['href']\n",
    "    print(article_title)\n",
    "    print(article_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Data From Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is obviously a fun service to get text from. We can use the **Tweepy** package to access the Twitter API. Before using Tweepy you must have Twitter **OAuth credentials** available from https://apps.twitter.com/. Create a new applciation (using your own Twitter credentials) and the generate access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OAuth access details for getting at the Twitter API - having these in my code is pretty insecure!!\n",
    "# consumer_key = \"ADD KEYS HERE\"\n",
    "# consumer_secret = \"ADD KEYS HERE\"\n",
    "# access_token = \"ADD KEYS HERE\"\n",
    "# access_token_secret = \"ADD KEYS HERE\" \n",
    "consumer_key = \"qNii6AnY3YxLfcf6ZJTauo6ry\"\n",
    "consumer_secret = \"SLQ2YB4NL05gWsaEZvO9lfuMxdQNVw0fNSUsKnjlLjLJe0drIS\"\n",
    "access_token = \"4876325422-uA89EDGIeAXIjIiddCyXS8cAoVWeFLqOinv4ahQ\"\n",
    "access_token_secret = \"ZXe7QV3u7BgBj46e1ZT8UFejzHLohZtDDavK3WTrLLiNh\"\n",
    "\n",
    "# Connect to the Twitter API using authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the tweets appearing in my timeline\n",
    "coord_list = list()\n",
    "public_tweets = api.home_timeline(count=25)\n",
    "for tweet in public_tweets:\n",
    "    print(\"@\" + tweet.author.screen_name, \"|\", tweet.author.name)\n",
    "    print(tweet.text)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the search API to find tweets based on a search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Search for recent tweets containing a specific keyword\n",
    "results = api.search(q=\"Dublin\", count=10)\n",
    "for tweet in results:\n",
    "    print(\"@\" + tweet.author.screen_name, \"|\", tweet.author.name)\n",
    "    print(tweet.text)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively find more and more tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id_found = None\n",
    "tweets_found = list()\n",
    "query_term = 'machine'\n",
    "for i in  range(0, 50):\n",
    "    if(max_id_found == None):\n",
    "        #Search for recent tweets containing a specific keyword\n",
    "        results = api.search(q=query_term, count=100)\n",
    "        max_id_found = results[-1].id\n",
    "    else:\n",
    "        #Search for recent tweets containing a specific keyword\n",
    "        results = api.search(q=query_term, count=100, max_id=max_id_found)\n",
    "        max_id_found = results[-1].id\n",
    "    \n",
    "    tweets_found = tweets_found + results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write tweets to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import codecs\n",
    "tweets_dir = \"data/tweets/\"\n",
    "if not os.path.exists(tweets_dir): os.mkdir(tweets_dir)\n",
    "for tweet in tweets_found:\n",
    "\n",
    "    try:\n",
    "        file = codecs.open(str(tweets_dir + tweet.id_str + \".txt\"),\"w\", \"utf-8\") \n",
    "        file.write(str(\"@\" + tweet.author.screen_name + \" \" + tweet.author.name))\n",
    "        file.write(tweet.text)\n",
    "        file.close() \n",
    "    except:\n",
    "        print('skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_found:\n",
    "    print(\"@\" + tweet.author.screen_name, \"|\", tweet.author.name)\n",
    "    print(tweet.text)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the tweets written out into an nltk corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_corpus = nltk.corpus.reader.plaintext.PlaintextCorpusReader(root='data/tweets', \\\n",
    "                                                                   fileids = '.*\\.txt', encoding= 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fd = nltk.FreqDist(tweets_corpus.words())\n",
    "tweet_fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a TweetTokeniser instead of the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_corpus = nltk.corpus.reader.plaintext.PlaintextCorpusReader(root='./data/tweets', \\\n",
    "                                                                   word_tokenizer = nltk.casual.TweetTokenizer(), \\\n",
    "                                                                   fileids = '.*\\.txt', encoding= 'utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fd = nltk.FreqDist(tweets_corpus.words())\n",
    "tweet_fd.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fd.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a unigram labnguage model for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_freq_dist = nltk.FreqDist(tweets_corpus.words())\n",
    "unigram_model = nltk.probability.LaplaceProbDist(tweet_freq_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a bigram labnguage model for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dist = nltk.probability.ConditionalFreqDist(\\\n",
    " nltk.bigrams(tweets_corpus.words(), pad_right=True, pad_left=True))\n",
    "\n",
    "bigram_model = nltk.probability.ConditionalProbDist(\\\n",
    "                    bigram_dist, \\\n",
    "                    nltk.probability.LaplaceProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a trigram language model for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = nltk.trigrams(tweets_corpus.words(), pad_right=True, pad_left=True)\n",
    "trigram_pairs = [((t[0], t[1]), t[2]) for t in trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdist = nltk.probability.ConditionalFreqDist(trigram_pairs)\n",
    "cpdist = nltk.probability.ConditionalProbDist(cfdist, nltk.probability.LaplaceProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = list()\n",
    "word1 = unigram_model.generate()\n",
    "word2 = bigram_model[word1].generate()\n",
    "sentence.append(word1)\n",
    "for i in range(0, 20):\n",
    "    sentence.append(word2)\n",
    "    word3 = cpdist[(word1, word2)].generate()\n",
    "    word1 = word2\n",
    "    word2 = word3\n",
    "display(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
