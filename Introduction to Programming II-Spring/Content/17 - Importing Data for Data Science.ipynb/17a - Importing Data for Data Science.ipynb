{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data for Data Science 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Data From Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing data from a text file is straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple text file into a string variable\n",
    "f = open(\"data/test_text.txt\", \"r\")\n",
    "# Separates text by new line character \\n. Reads until EOF. Equivalently you can use list(f)\n",
    "lines = f.readlines()\n",
    "# Remember to free up the resources used by the file when you are finished with it!\n",
    "f.close()\n",
    "# Iterate through each line in the file and print it out\n",
    "for line in lines:\n",
    "    print(\"****\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    print(\"****\")\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even load a text file across the Internet by using **requests.get** from the **requests** package instead of simply **open**. We use [the Guttenberg Press](http://www.gutenberg.org) in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the requests package\n",
    "import requests\n",
    "\n",
    "# Define a URL to Alice in Wonderland on the Guttenberg Press (www.gutenberg.org)\n",
    "url='http://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "\n",
    "# Read the text from the URL. This variable is just a long string. \n",
    "text_page = requests.get(url).text\n",
    "# Print the first 1000 characters of the book\n",
    "print(text_page[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even connect to a HTML file, but this starts to get really hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to a URL and extract the HTML text\n",
    "# url = \"http://www.independent.ie/sport/soccer/international-soccer/neil-taylor-facing-longer-ban-for-seamus-coleman-horror-tackle-as-fifa-step-in-35578919.html\"\n",
    "url = \"https://www.rte.ie/news/2019/0410/1041742-eu_brexit_summit/\"\n",
    "text = requests.get(url).text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing data from web pages is straightforward. The tricky bit is extracting the useful information from the webpage. We can use the **BeautifulSoup4** (http://www.crummy.com/software/BeautifulSoup) packages to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the BeautifulSoup package\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# Read the HTML file\n",
    "# url = \"http://www.independent.ie/sport/soccer/international-soccer/neil-taylor-facing-longer-ban-for-seamus-coleman-horror-tackle-as-fifa-step-in-35578919.html\"\n",
    "url = \"https://www.rte.ie/news/2019/0410/1041742-eu_brexit_summit/\"\n",
    "html = requests.get(url).text\n",
    "\n",
    "# Create a beautiful soup object from the text file so that we get at the article text\n",
    "article_soup = BeautifulSoup(html, \"lxml\")\n",
    "# print(article_soup)\n",
    "# Extract the actual article text  - this relies on the fact that I know what the HTML looks like, not completely robust!\n",
    "article = article_soup.find('article')\n",
    "# Gives us text including tags. \n",
    "headline = article.find('h1')\n",
    "# This is out of date now\n",
    "# article_content = article.find_all('div', class_='ctx_content')\n",
    "article_content = article.find_all('section', class_=\"medium-10 medium-offset-1 columns article-body\")\n",
    "\n",
    "# Raw data\n",
    "# print(article_content)\n",
    "\n",
    "# Start the article text by adding the headline (this will get what's in between the tags)\n",
    "article_text = headline.get_text()\n",
    "\n",
    "# Construct the article by adding together the paragraph pieces\n",
    "for tag in article_content:\n",
    "    article_text += tag.get_text()\n",
    "    \n",
    "# Print the article content\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wordcloud is a fun way to visualise text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import package for drawing word clouds this needs to be installed from binstar using ./conda install -c https://conda.binstar.org/derickl wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# Create a word cloud\n",
    "# Because everyone loves comic sans right? - This font path will probably only work on Mac. \n",
    "wordcloud = WordCloud(font_path='/Library/Fonts/Comic Sans MS.ttf',\n",
    "                     stopwords=STOPWORDS,\n",
    "                      background_color='white',\n",
    "                      width=2400,\n",
    "                      height=2400\n",
    "                     ).generate(article_text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing RSS Feeds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to access lots of news articles is to use an RSS (Really Simple Syndication) feed. We can access RSS feeds easily in Python using the **feedparser** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For reading RSS feeds - imstnall using ./conda install feedparser\n",
    "import feedparser \n",
    "\n",
    "# Read from the Irish Times RSS feed\n",
    "RSS_url = \"https://www.irishtimes.com/cmlink/news-1.1319192\"\n",
    "it_feed = feedparser.parse(RSS_url)\n",
    "print(\"Number of entries:\", len(it_feed.entries))\n",
    "\n",
    "# Iterate through the entries from the feed and print the title of each article and the URL for the articl\n",
    "for article_entry in it_feed.entries:\n",
    "    article_title = article_entry['title']\n",
    "    article_url = article_entry['links'][0]['href']\n",
    "    print(article_title)\n",
    "    print(article_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Data From Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is obviously a fun service to get text from. We can use the **Tweepy** package to access the Twitter API. Before using Tweepy you must have Twitter **OAuth credentials** available from https://apps.twitter.com/. Create a new applciation (using your own Twitter credentials) and the generate access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tweepy \n",
    "import tweepy\n",
    "\n",
    "# OAuth access details for getting at the Twitter API - having these in my code is pretty insecure!!\n",
    "consumer_key = \"ENTER YOUR KEY HERE\"\n",
    "consumer_secret = \"ENTER YOUR SECRET HERE\"\n",
    "access_token = \"ENTER YOUR TOKEN HERE\"\n",
    "access_token_secret = \"ENTER YOUR TOKEN HERE\" \n",
    "\n",
    "# Connect to the Twitter API using authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Access the tweets appearing in my timeline\n",
    "coord_list = list()\n",
    "public_tweets = api.home_timeline(count=25)\n",
    "for tweet in public_tweets:\n",
    "    print(\"@\" + tweet.author.screen_name, \"|\", tweet.author.name)\n",
    "    print(tweet.text)\n",
    "    print() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Search for recent tweets containing a specific keyword\n",
    "results = api.search(q=\"Dublin\", count=10)\n",
    "for tweet in results:\n",
    "    print(\"@\" + tweet.author.screen_name, \"|\", tweet.author.name)\n",
    "    print(tweet.text)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction To NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Natural Language Toolkit**, **NLTK** (http://www.nltk.org/), is a well written, widely used, and well respected toolkit for performing natural language processing in Python. It offers a wide range of useful functionality and data structres that make text natural language processing, and so text analytics, much easier. Features included in the NLTK include corpus management, document classification, colocation discovery, part of speech tagging, parsing, and chunking. The best reference for the NLTK is the **NLTK Book**, Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit by Steven Bird, Ewan Klein, and Edward Loper, which is freely avialble online at http://www.nltk.org/book/ or for sale at http://www.amazon.co.uk/Natural-Language-Processing-Python-Steven/dp/0596516495. Many of the examples in this tutorial are taken from this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a set of packages that we will use in order to perform text analysis. These are very commonly used Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # The best known Python natural language processing toolkit\n",
    "from nltk import FreqDist # Explicitlty import the FreqDist function from NLTK\n",
    "import numpy # Package for scientific computing\n",
    "import matplotlib # Python plotting library\n",
    "import matplotlib.pyplot as plt # Easy syntax access to pyplot\n",
    "import re # functions fior dealing with regular expressions\n",
    "from wordcloud import WordCloud, STOPWORDS # package for drawing word clouds this needs to be installed from binstar using ./conda install -c https://conda.binstar.org/derickl wordcloud\n",
    "from urllib.request import urlopen # for accessing URLs\n",
    "from bs4 import BeautifulSoup # For parsing HTML documents\n",
    "\n",
    "# Tells iPython notebook to draw graphic sinline in the webpage\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this in order to launch the NLTK downloader to access corpora, packages etc\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load An NLTK Built-In Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Text Within a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concordance returns a set of sentences that include a search term or terms. We first create an NLTK text object that we can manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = nltk.Text(nltk.corpus.gutenberg.words('shakespeare-hamlet.txt'))\n",
    "print(hamlet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NLTK **concordance** function we can generate a set of setences containing a chose word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet.concordance(\"ophelia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a text object containing all of the texts in our corpus and generate a concordance from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allText = nltk.Text(nltk.corpus.gutenberg.words())\n",
    "# \"A concordance view shows us every occurrence of a given word, together with some context.\"\n",
    "# https://www.nltk.org/book/ch01.html\n",
    "allText.concordance('sandwich')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Generate a concordance of the occurences of the word *whale* in *Moby Dick* and the word *computer* in the overall gutenburg corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick = nltk.Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "moby_dick .concordance('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText.concordance('computer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **dispersion plot** is a fun data visualsiation supported by NLTK that shows us where in a text words appear. It is generated using the **dispersion_plot** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet.dispersion_plot([\"Hamlet\", \"Horatio\", \"Ophelia\", \"Fortinbras\", \"Yorick\", \"death\", \"skull\", \"dagger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more powerful way that we can find text within a corpus is to use **regular expressions**. Regular expressions are a powerful way to define textual patterns that allow us find interesting things within a document. First just find all words ending in \"ings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(set([w for w in hamlet if re.search('ings$', w)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is some nice Python code to iterate through all the words in our hamlet list, and to add those that match our regular expression to a new list. Python is great for this type of stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example we consider a Hamlet-based crossword puzzle in which we we need to find a word that matches this pattern: \\_ \\_ m \\_ \\_ t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " list(set([w for w in hamlet if re.search('^..m..t$', w)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is tonnes that you can do with regular expressions - find dates, find phone numbers, find matches for types of words, find pattrerns across multiple words .... The basic operators for definining regular expressions are as follows.\n",
    "\n",
    "Operator | Behavior\n",
    "----------|------------\n",
    ". |\tWildcard, matches any character\n",
    "^abc |\tMatches some pattern abc at the start of a string\n",
    "abc\\$ |\tMatches some pattern abc at the end of a string\n",
    "[abc] |\tMatches one of a set of characters\n",
    "[A-Z0-9] |\tMatches one of a range of characters\n",
    "ed $\\mid$ ing $\\mid$ s |\tMatches one of the specified strings (disjunction)\n",
    "* |\tZero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
    "+ |\tOne or more of previous item, e.g. a+, [a-z]+\n",
    "? |\tZero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
    "{n} |\tExactly n repeats where n is a non-negative integer\n",
    "{n,} |\tAt least n repeats\n",
    "{,n} |\tNo more than n repeats\n",
    "{m,n} |\tAt least m and no more than n repeats\n",
    "a(b $\\mid$ c)+ |\tParentheses that indicate the scope of the operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Load the corpus of American presedential inaugural addresses, nltk.corpus.inaugural, and find all mentions of *America*, *freedom*, and *war*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural_add = nltk.Text(nltk.corpus.inaugural.words())\n",
    "inaugural_add.dispersion_plot([\"America\",\"freedom\",\"war\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting vocabulary is a really important thing to do in text, and we can do it easily in Python with NLTK. First, let's get the number of words in Hamlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hamlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the number of unique words - converting from a Python **list** to a Python **set** does this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set(hamlet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexical diversity** is a technique used to measure how complicated a text is. It is just the ratio of unique words to total words. Higher values indicate more complicated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set to get unique values which we then count\n",
    "len(set(hamlet))/len(hamlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Calculate the lexical diversity of *Moby Dick* and *Alice In Wonderland*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.Text(nltk.corpus.gutenberg.words('carroll-alice.txt'))\n",
    "len(set(alice))/len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(moby_dick))/len(moby_dick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily define a lexical diversity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calcualte lexical diversity\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "# Use the newly defined function to claculate the lexical diversity of hamlet\n",
    "lexical_diversity(hamlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the **most frequently occuring words** in a text is one of the most common ways to analyse the meaning of a text. The NLTK makes this job very easy for us by allowing us to quickly generate a **frequency distribution** using the **FreqDist** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# convert all words to lower case\n",
    "hamlet = [w.lower() for w in hamlet]\n",
    "\n",
    "# Remove all punctuation from word lists - note the use of regular expressions!\n",
    "# If the output of the regular expression that is looking for the pattern is None,\n",
    "# add to the list. \n",
    "hamlet = [w for w in hamlet if not (re.match(r'^\\W+$', w) != None)]\n",
    "\n",
    "# Remove all stop words from word lists\n",
    "hamlet = [w for w in hamlet if not w in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "#print hamlet\n",
    "\n",
    "# Print revised lexical diversity\n",
    "#print '{}{}'.format('Lexical diversity: ', lexical_diversity(hamlet))\n",
    "\n",
    "# Generate the frquency distribution for hamlet\n",
    "hamlet_freq_dist = nltk.FreqDist(hamlet)\n",
    "print(hamlet_freq_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of occurences of the word hamlet\n",
    "print(hamlet_freq_dist['hamlet'])\n",
    "print('{}{}'.format('Frequency Hamlet: ', hamlet_freq_dist['hamlet']))\n",
    "\n",
    "# Print the top X words\n",
    "numWords = 0\n",
    "wordLimit = 20\n",
    "for w in hamlet_freq_dist.keys():\n",
    "    print('{}{}{}'.format(w, ': ', hamlet_freq_dist[w]))\n",
    "    numWords = numWords + 1\n",
    "    if numWords > wordLimit:\n",
    "        break\n",
    "\n",
    "# Plot a nice graph of word frequencies\n",
    "hamlet_freq_dist.plot(wordLimit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot a word cloud from this frequency distribution (watch out for the path to the font used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path='/Library/Fonts/Comic Sans MS.ttf',\n",
    "                     stopwords=STOPWORDS,\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(str(hamlet))\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "#matplotlib.savefig('./my_twitter_wordcloud_1.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice webpage explaining how to make word clouds from your Twitter feed: http://spartanideas.msu.edu/2014/11/28/turn-your-twitter-timeline-into-a-word-cloud-using-python/#A.-Downloading-Your-Twitter-Timeline-Tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
